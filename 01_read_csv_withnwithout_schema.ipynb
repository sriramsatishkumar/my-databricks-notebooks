{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d488054-2fe9-45dd-ae31-b9c711420718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "Here is in this notebook you can find how to read a **CSV** file.<br>\n",
    "Using **inferSchema** and using a **structType**<br>\n",
    "explore **pyspark.sql.Types** libraries to import while using **structType** and **structField**<br>\n",
    "This notebook is written in **Python** so the default cell type is Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2964446d-d4cf-49b5-babc-b0f319606b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/tables/addresses.csv', name='addresses.csv', size=397, modificationTime=1727409570000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/baby_names.csv', name='baby_names.csv', size=7447879, modificationTime=1726971369000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/basketball.csv', name='basketball.csv', size=1352, modificationTime=1727410075000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/biostats1.csv', name='biostats1.csv', size=331, modificationTime=1727467750000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/fs_test/', name='fs_test/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/mv_files/', name='mv_files/', size=0, modificationTime=0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c968113e-f8ab-4cdb-b178-e838ebd879ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Height: integer (nullable = true)\n |-- Weight: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\")\\\n",
    "\t.option(\"inferschema\", True)\\\n",
    "\t.option(\"sep\", \",\")\\\n",
    "\t.option(\"header\", True)\\\n",
    "\t.load(\"/FileStore/tables/biostats1.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de846bac-3326-41ee-b1ce-a2e63487da2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format(\"csv\").options(inferschema=\"True\", sep=\",\", header =\"True\").load(\"/FileStore/tables/biostats1.csv\")\n",
    "\n",
    "#display(df1)\n",
    "print(df1.count())\n",
    "#df1.printSchema()\n",
    "df1.createOrReplaceTempView(\"biostats1_infer\")\n",
    "#spark.sql(\"select * from biostats1_infer\").display()\n",
    "df1.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"biostats1_infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4bae62-facf-4bd7-8d52-671ac74aa83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,DecimalType\n",
    "\n",
    "schema_defined = StructType([StructField('Name', StringType(), True),\n",
    "                             StructField('Sex', StringType(), True),\n",
    "                             StructField('Age', DecimalType(), True),\n",
    "                             StructField('Height', DecimalType(), True),\n",
    "                             StructField('Weight', DecimalType(), True)\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d4eb06-716a-47ac-b3d7-d80f8eacc1e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\").schema(schema_defined).options(header=\"True\",sep=\",\").load(\"/FileStore/tables/biostats1.csv\")\n",
    "\n",
    "df2.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"biostats1_schemadef\")\n",
    "\n",
    "#df2.printSchema()  \n",
    "#display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efe7851-bfe4-4c3b-b932-b319590b2ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n|Name|Sex|Age|Height|Weight|\n+----+---+---+------+------+\n|Alex|  M| 41|    74|   170|\n|Bert|  M| 42|    68|   166|\n|Carl|  M| 32|    70|   155|\n|Dave|  M| 39|    72|   167|\n|Elly|  F| 30|    66|   124|\n|Fran|  F| 33|    66|   115|\n|Gwen|  F| 26|    64|   121|\n|Hank|  M| 30|    71|   158|\n|Ivan|  M| 53|    72|   175|\n|Jake|  M| 32|    69|   143|\n|Kate|  F| 47|    69|   139|\n|Luke|  M| 34|    72|   163|\n|Myra|  F| 23|    62|    98|\n|Neil|  M| 36|    75|   160|\n|Omar|  M| 38|    70|   145|\n|Page|  F| 31|    67|   135|\n|Quin|  M| 29|    71|   176|\n|Ruth|  F| 28|    65|   131|\n+----+---+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.biostats1_schemadef\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce43929-4c45-42d2-9472-a7c0fe262e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_new = 'Name STRING, Sex STRING, Age INTEGER, Height INTEGER, Weight INTEGER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bebdf41-ae42-4407-9c89-1973636f7a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"csv\").schema(schema_new).options(header=\"True\",sep=\",\").load(\"/FileStore/tables/biostats1.csv\")\n",
    "\n",
    "df3.createOrReplaceTempView(\"biostats1_schemanew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb8b133-5d1a-429f-a3be-7975c9e3af03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.catalyst.parser.ParseException: \n",
       "[PARSE_SYNTAX_ERROR] Syntax error at or near '%'. SQLSTATE: 42601 (line 2, pos 0)\n",
       "\n",
       "== SQL ==\n",
       "drop table if exists default.biostats1_schemanew\n",
       "%run _sqldf\n",
       "^^^\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$7(SparkSession.scala:952)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:471)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$6(SparkSession.scala:951)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withMainTracker(QueryPlanningTracker.scala:188)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:950)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1184)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:950)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:986)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:300)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:400)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:422)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:417)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:981)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1163)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1154)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:95)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1098)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1510)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:775)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:931)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:920)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:952)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:784)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:586)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[PARSE_SYNTAX_ERROR] Syntax error at or near '%'. SQLSTATE: 42601"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PARSE_SYNTAX_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42601",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '%'. SQLSTATE: 42601 (line 2, pos 0)\n\n== SQL ==\ndrop table if exists default.biostats1_schemanew\n%run _sqldf\n^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$7(SparkSession.scala:952)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:471)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$6(SparkSession.scala:951)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withMainTracker(QueryPlanningTracker.scala:188)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:950)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1184)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:950)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:986)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:300)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:400)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:422)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:417)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:981)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1163)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1154)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:95)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1098)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1510)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:775)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:931)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:920)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:952)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:717)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:784)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:512)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:306)\n\tat java.lang.Thread.run(Thread.java:750)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table if exists default.biostats1_schemanew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6675dffa-22d5-48a0-8287-cac06bedd1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n|Name|Sex|Age|Height|Weight|\n+----+---+---+------+------+\n|Alex|  M| 41|    74|   170|\n|Bert|  M| 42|    68|   166|\n|Carl|  M| 32|    70|   155|\n|Dave|  M| 39|    72|   167|\n|Elly|  F| 30|    66|   124|\n|Fran|  F| 33|    66|   115|\n|Gwen|  F| 26|    64|   121|\n|Hank|  M| 30|    71|   158|\n|Ivan|  M| 53|    72|   175|\n|Jake|  M| 32|    69|   143|\n|Kate|  F| 47|    69|   139|\n|Luke|  M| 34|    72|   163|\n|Myra|  F| 23|    62|    98|\n|Neil|  M| 36|    75|   160|\n|Omar|  M| 38|    70|   145|\n|Page|  F| 31|    67|   135|\n|Quin|  M| 29|    71|   176|\n|Ruth|  F| 28|    65|   131|\n+----+---+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table default.biostats1_schemanew using parquet as select * from biostats1_schemanew\")\n",
    "spark.sql(\"select * from default.biostats1_schemanew\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4138738123866937,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_read_csv_withnwithout_schema",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
